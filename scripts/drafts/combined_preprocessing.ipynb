{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11398b2-8f9d-4afa-990f-00e534a42fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up all of the packages!\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import rasterio\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from utils import * \n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 400\n",
    "plt.rcParams['font.size'] = 13\n",
    "plt.rcParams[\"legend.frameon\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31729c64-614d-4538-8302-b820de1bc472",
   "metadata": {},
   "source": [
    "## Import and prepare our predictor and predictant datasets\n",
    "predictor: (X, TerraClim climate variables), predictant: (Y, MODIS LAI) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fade918-a80f-4f25-8fed-3c0ea050f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish path and open as an array\n",
    "path = \"/home/jovyan/large_files/climLai_merged.nc\"\n",
    "ds_combined = xr.open_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d400b9-e8af-4521-9f59-62a20ba8ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 181MB\n",
      "Dimensions:      (time: 228, lat: 107, lon: 116)\n",
      "Coordinates:\n",
      "  * time         (time) datetime64[ns] 2kB 2002-01-31 2002-02-28 ... 2020-12-31\n",
      "  * lat          (lat) float32 428B -9.125 -9.175 -9.225 ... -14.38 -14.43\n",
      "  * lon          (lon) float32 464B -73.47 -73.42 -73.38 ... -67.78 -67.72\n",
      "Data variables:\n",
      "    spatial_ref  int64 8B ...\n",
      "    tmmx         (time, lat, lon) float64 23MB ...\n",
      "    tmmn         (time, lat, lon) float64 23MB ...\n",
      "    pr           (time, lat, lon) float64 23MB ...\n",
      "    pdsi         (time, lat, lon) float64 23MB ...\n",
      "    def          (time, lat, lon) float64 23MB ...\n",
      "    vpd          (time, lat, lon) float64 23MB ...\n",
      "    soil         (time, lat, lon) float64 23MB ...\n",
      "    lai          (time, lat, lon) float64 23MB ...\n"
     ]
    }
   ],
   "source": [
    "# take a look at the ds_combined dataset\n",
    "print(ds_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d093043-fe30-4d4d-bbbb-9049df5572fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data array has 3 dimensions. Stack the coordinate values to go from 3d to 2d.\n",
    "ds_stacked = ds_combined.stack(stacked=(\"lat\", \"lon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ceae19e-15f8-495a-b97a-988f7d9c1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to data frame\n",
    "df_combined = ds_stacked.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9dfc77-c11f-49e9-ae8a-29a7d781048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe generate the x_df (ONLY climate predictor variables, drop the lai column)\n",
    "x_df = df_combined.drop(columns=[\"lai\",\"spatial_ref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0501f25-6335-413c-978d-1c3b5715f8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tmmx</th>\n",
       "      <th>tmmn</th>\n",
       "      <th>pr</th>\n",
       "      <th>pdsi</th>\n",
       "      <th>def</th>\n",
       "      <th>vpd</th>\n",
       "      <th>soil</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2002-01-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">-9.125002</th>\n",
       "      <th>-73.474998</th>\n",
       "      <td>32.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>359.0</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>156.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.424995</th>\n",
       "      <td>32.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>361.0</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.375000</th>\n",
       "      <td>31.9</td>\n",
       "      <td>22.1</td>\n",
       "      <td>370.0</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>154.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.324997</th>\n",
       "      <td>31.9</td>\n",
       "      <td>22.1</td>\n",
       "      <td>371.0</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>153.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.275002</th>\n",
       "      <td>31.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>375.0</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>152.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tmmx  tmmn     pr  pdsi  def   vpd   soil\n",
       "time       lat       lon                                                  \n",
       "2002-01-31 -9.125002 -73.474998  32.1  22.2  359.0 -1.68  0.0  0.89  156.8\n",
       "                     -73.424995  32.0  22.2  361.0 -1.65  0.0  0.88  156.0\n",
       "                     -73.375000  31.9  22.1  370.0 -1.57  0.0  0.87  154.5\n",
       "                     -73.324997  31.9  22.1  371.0 -1.52  0.0  0.86  153.7\n",
       "                     -73.275002  31.8  22.1  375.0 -1.48  0.0  0.85  152.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at x_df\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93578231-30ea-4a2e-97f9-c4de508f5c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lai</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2002-01-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">-9.125002</th>\n",
       "      <th>-73.474998</th>\n",
       "      <td>4.803474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.424995</th>\n",
       "      <td>5.169796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.375000</th>\n",
       "      <td>5.366670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.324997</th>\n",
       "      <td>5.473258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-73.275002</th>\n",
       "      <td>5.366331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      lai\n",
       "time       lat       lon                 \n",
       "2002-01-31 -9.125002 -73.474998  4.803474\n",
       "                     -73.424995  5.169796\n",
       "                     -73.375000  5.366670\n",
       "                     -73.324997  5.473258\n",
       "                     -73.275002  5.366331"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dataframe generate y_df (ONLY lai)\n",
    "y_df = df_combined.drop(columns=[\"spatial_ref\",\n",
    "                                 \"tmmx\",\n",
    "                                 \"tmmn\",\n",
    "                                 \"pr\",\n",
    "                                 \"pdsi\",\n",
    "                                 \"def\",\n",
    "                                 \"vpd\",\n",
    "                                 \"soil\"])\n",
    "# look at y_df\n",
    "y_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f32dd-8eb7-4346-836e-5654071e2a6a",
   "metadata": {},
   "source": [
    "## Split our data into training and validation\n",
    "Look at the entire time sequence of our data: 19 years, 12 months per year\n",
    "(19yrs x 12months) = 228 time steps\n",
    "\n",
    "The first 15 years are our training data: 2002 - 2016 (15yrs x 12months = 180 ts)\n",
    "The last 4 years are our validation data: 2017 - 2020 (4yrs x 12 months = 48 ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c5b4f9-1eea-4f1f-b077-5c9a9527c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0 = 2002-01-31 00:00:00\n",
      "time last = 2020-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# look at the entire time sequence of our data\n",
    "first_time = x_df.index.get_level_values(\"time\")[0]\n",
    "last_time  = x_df.index.get_level_values(\"time\")[-1]\n",
    "\n",
    "print('time 0 =', first_time)\n",
    "print('time last =', last_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f15bef-ac7c-4e9c-bc9d-0e2919154ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data according to time splits above\n",
    "\n",
    "# establish the before and after\n",
    "times = x_df.index.get_level_values(\"time\")\n",
    "mask_before = times < '2017-01-31 00:00:00'\n",
    "mask_after  = times >= '2017-01-31 00:00:00'\n",
    "\n",
    "# x training validation data split\n",
    "x_df_train = x_df.loc[mask_before]\n",
    "x_df_valid  = x_df.loc[mask_after]\n",
    "\n",
    "# y training validation data split\n",
    "y_df_train = y_df.loc[mask_before]\n",
    "y_df_valid = y_df.loc[mask_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcca6df-61d0-4152-9d94-b8ff1c0dafff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape x_df_train (2234160, 7)\n",
      "shape x_df_valid (595776, 7)\n",
      "shape y_df_train (2234160, 1)\n",
      "shape y_df_valid (595776, 1)\n"
     ]
    }
   ],
   "source": [
    "# test to make sure the splits worked:\n",
    "print('shape x_df_train', x_df_train.shape)\n",
    "print('shape x_df_valid', x_df_valid.shape)\n",
    "\n",
    "print('shape y_df_train', y_df_train.shape)\n",
    "print('shape y_df_valid', y_df_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab16b1-2af7-40ea-be04-81e747d979a5",
   "metadata": {},
   "source": [
    "# SKIP THIS! WE SCALE LATER ON! AND BECAUSE WE ARE STANDARDIZING ON SEPERATE TRAINING AND VALIDATION WE ARE INTRODUCING BIAS. THIS IS WRONG. \n",
    "## Standardize our data to prepare it for LSTM ingestion\n",
    "\n",
    "Standardize our data to prepare it for LSTM ingestion :o\n",
    "We want to standardize per location, to avoid losing any spatial relationships\n",
    "\n",
    "There is a question here of whether we should be standardizing the y values because NDVI is already a constrained abd bounded value <-- much to think about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3788580-99b6-444d-8ad6-46e2bd899ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1065/301446771.py:21: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_df_train_sd = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n"
     ]
    }
   ],
   "source": [
    "# standardizing our y_df_train values\n",
    "def standardize_per_location(y_df_train, feature_col=\"lai\"):\n",
    "    # Extract levels\n",
    "    idx = y_df_train.index\n",
    "\n",
    "    # Create a DataFrame from index levels for grouping\n",
    "    idx_df = pd.DataFrame({\n",
    "        'time': idx.get_level_values('time'),\n",
    "        'lat': idx.get_level_values('lat'),\n",
    "        'lon': idx.get_level_values('lon'),\n",
    "        feature_col: y_df_train[feature_col].values\n",
    "    })\n",
    "\n",
    "    # Group by location (lat, lon)\n",
    "    def standardize_group(group):\n",
    "        vals = group[feature_col]\n",
    "        standardized_vals = (vals - vals.mean()) / vals.std()\n",
    "        group[feature_col] = standardized_vals\n",
    "        return group\n",
    "\n",
    "    y_df_train_sd = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n",
    "\n",
    "    # Restore MultiIndex\n",
    "    y_df_train_sd.set_index(['time', 'lat', 'lon'], inplace=True)\n",
    "    y_df_train_sd = y_df_train_sd.sort_index()\n",
    "\n",
    "    return y_df_train_sd[[feature_col]]\n",
    "\n",
    "# Apply\n",
    "y_df_train_sd = standardize_per_location(y_df_train, feature_col=\"lai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bdeea57-7b22-4de1-a178-30b96b6e3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1065/680808079.py:21: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_df_valid_sd = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n"
     ]
    }
   ],
   "source": [
    "# standardizing our y_df_valid values\n",
    "def standardize_per_location(y_df_valid, feature_col=\"lai\"):\n",
    "    # Extract levels\n",
    "    idx = y_df_valid.index\n",
    "\n",
    "    # Create a DataFrame from index levels for grouping\n",
    "    idx_df = pd.DataFrame({\n",
    "        'time': idx.get_level_values('time'),\n",
    "        'lat': idx.get_level_values('lat'),\n",
    "        'lon': idx.get_level_values('lon'),\n",
    "        feature_col: y_df_valid[feature_col].values\n",
    "    })\n",
    "\n",
    "    # Group by location (lat, lon)\n",
    "    def standardize_group(group):\n",
    "        vals = group[feature_col]\n",
    "        standardized_vals = (vals - vals.mean()) / vals.std()\n",
    "        group[feature_col] = standardized_vals\n",
    "        return group\n",
    "\n",
    "    y_df_valid_sd = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n",
    "\n",
    "    # Restore MultiIndex\n",
    "    y_df_valid_sd.set_index(['time', 'lat', 'lon'], inplace=True)\n",
    "    y_df_valid_sd = y_df_valid_sd.sort_index()\n",
    "\n",
    "    return y_df_valid_sd[[feature_col]]\n",
    "\n",
    "# Apply\n",
    "y_df_valid_sd = standardize_per_location(y_df_valid, feature_col=\"lai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7490ae8b-0562-4475-9904-3121e0f209a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1065/2995428060.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  standardized_df = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n",
      "/tmp/ipykernel_1065/2995428060.py:34: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  standardized_df = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n"
     ]
    }
   ],
   "source": [
    "def standardize_per_location(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Standardize multiple feature columns per location (lat, lon) over time.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with MultiIndex (time, lat, lon)\n",
    "    - feature_cols: list of feature column names to standardize\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with standardized features, same MultiIndex\n",
    "    \"\"\"\n",
    "\n",
    "    idx = df.index\n",
    "\n",
    "    # Build a DataFrame from the MultiIndex levels and feature columns\n",
    "    data = {\n",
    "        'time': idx.get_level_values('time'),\n",
    "        'lat': idx.get_level_values('lat'),\n",
    "        'lon': idx.get_level_values('lon')\n",
    "    }\n",
    "    # Add all feature columns to the data dict\n",
    "    for col in feature_cols:\n",
    "        data[col] = df[col].values\n",
    "\n",
    "    idx_df = pd.DataFrame(data)\n",
    "\n",
    "    # Group by location (lat, lon) and standardize each feature column\n",
    "    def standardize_group(group):\n",
    "        for col in feature_cols:\n",
    "            vals = group[col]\n",
    "            group[col] = (vals - vals.mean()) / vals.std()\n",
    "        return group\n",
    "\n",
    "    standardized_df = idx_df.groupby(['lat', 'lon']).apply(standardize_group)\n",
    "\n",
    "    # Restore MultiIndex and sort\n",
    "    standardized_df.set_index(['time', 'lat', 'lon'], inplace=True)\n",
    "    standardized_df = standardized_df.sort_index()\n",
    "\n",
    "    # Return only the standardized feature columns (MultiIndex preserved)\n",
    "    return standardized_df[feature_cols]\n",
    "\n",
    "\n",
    "feature_columns = [\"tmmx\", \"tmmn\", \"pr\", \"pdsi\", \"def\", \"vpd\", \"soil\"]  # your features\n",
    "x_df_train_sd = standardize_per_location(x_df_train, feature_columns)\n",
    "x_df_valid_sd = standardize_per_location(x_df_valid, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc94d89-06d1-4133-9911-e2d5bb43766c",
   "metadata": {},
   "source": [
    "## Reshape data to feed into the LSTM model\n",
    "okay let's do this!! >:O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "352cb35c-1877-4c3c-bcea-ebf8fcd45415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our missing packages!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a484980d-8927-4b67-bb58-54b1f72f403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - Ensure Time is a column of our data\n",
    "# Time is currently an INDEX not a column of our data, we need to convert it into a COLUMN\n",
    "def ensure_time_column(df):\n",
    "    \"\"\"\n",
    "    If time is the index, convert it into a column called 'time'.\n",
    "    Works for both training and validation dataframes.\n",
    "    \"\"\"\n",
    "    if \"time\" not in df.columns:\n",
    "        # index must be time\n",
    "        df = df.reset_index().rename(columns={\"index\": \"time\"})\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# 1 - Merge the X (predictors, climate variables) and Y (predictands, modis lai) datasets\n",
    "def merge_xy(x_df, y_df):\n",
    "    x_df = ensure_time_column(x_df)\n",
    "    y_df = ensure_time_column(y_df)\n",
    "    \n",
    "    df = x_df.merge(y_df, on=[\"time\", \"lat\", \"lon\"], how=\"inner\")\n",
    "    df = df.sort_values([\"lat\", \"lon\", \"time\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_df = merge_xy(x_df_train, y_df_train)\n",
    "valid_df = merge_xy(x_df_valid, y_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec55446-c07a-4415-a0b6-97bd1d47077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. scale the climate inputs, DO NOT DO THIS IF YOU HAVE PROPERLY STANDARDIZED\n",
    "# in this case we have not properly standardized, so we will scale\n",
    "# find the mean and std of each feature in the training dataset and standardize\n",
    "# then use the mean and std of the TRAINING data to transform the validation data\n",
    "feature_cols = [\"tmmx\", \"tmmn\", \"pr\", \"pdsi\", \"def\", \"vpd\", \"soil\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols]) # TRAINING DATA, CALL THE FIT_TRANSFORM\n",
    "valid_df[feature_cols] = scaler.transform(valid_df[feature_cols]) # VALIDATION DATA, CALL THE TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d19e1c54-8138-44c9-a865-331db4e7ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (2159688, 6, 7)\n",
      "Valid X shape: (521304, 6, 7)\n"
     ]
    }
   ],
   "source": [
    "# 3 build the lstm sequences for each pixel, establish 6 timesteps (months) as our \"look-back\" window\n",
    "def build_sequences(df, feature_cols, lookback):\n",
    "    X_list, y_list, coords = [], [], []\n",
    "\n",
    "    for (lat, lon), group in df.groupby([\"lat\", \"lon\"]):\n",
    "        group = group.sort_values(\"time\")\n",
    "\n",
    "        X_vals = group[feature_cols].values\n",
    "        y_vals = group[\"lai\"].values\n",
    "\n",
    "        for i in range(len(group) - lookback):\n",
    "            X_list.append(X_vals[i:i+lookback])\n",
    "            y_list.append(y_vals[i+lookback])\n",
    "            coords.append((lat, lon))\n",
    "\n",
    "    return (\n",
    "        np.array(X_list, dtype=np.float32),\n",
    "        np.array(y_list, dtype=np.float32),\n",
    "        coords,\n",
    "    )\n",
    "\n",
    "\n",
    "lookback = 6 # will use the past 6 months (time-steps of data) to get the next 7th month\n",
    "\n",
    "X_train_np, y_train_np, train_coords = build_sequences(train_df, feature_cols, lookback)\n",
    "X_valid_np, y_valid_np, valid_coords = build_sequences(valid_df, feature_cols, lookback)\n",
    "\n",
    "print(\"Train X shape:\", X_train_np.shape) # (len_t), lookback, feat\n",
    "print(\"Valid X shape:\", X_valid_np.shape) # (len_v), lookback, feat\n",
    "\n",
    "# Train X shape and Valid X shape should have the same lookback and features (the last two values)\n",
    "# BUT should differ in their first domension, with Train beging larger reflecting there is a greater ammount of data going into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36cdfc81-eea8-41d3-bd0d-937f698b6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 convert to PYTORCH dataset, ready for model ingestion\n",
    "class ClimateLAIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ClimateLAIDataset(X_train_np, y_train_np)\n",
    "valid_dataset = ClimateLAIDataset(X_valid_np, y_valid_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6434444-7190-4ac2-8f03-c49ce2bc6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 split into training, validation, testing data\n",
    "# current data is split 80% to training, 20% to validation\n",
    "# we will split the 20% of validation into 10% validation, 10% testing\n",
    "\n",
    "# Split our validation into valid and test datasets\n",
    "#valid_size = int(0.1 * len(valid_dataset))\n",
    "#test_size = len(valid_dataset) - valid_size\n",
    "#valid_dataset, test_dataset = torch.utils.data.random_split(valid_dataset, [valid_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef50053-0ce9-45a5-afea-6b208f7fccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 DATALOADERS, both should be shuffle=false, according to Pierre\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a846a0-d85b-412e-be2a-ff13f332ce5c",
   "metadata": {},
   "source": [
    "## Define LSTM structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d7afdff-444c-453a-b6ec-6ce34f68ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'ReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cc7c04f-8631-4bbd-ac83-bb0a8a58aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f74e87dd-52f4-49b9-bd50-f4d557396159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAI_LSTM(\n",
       "  (lstm): LSTM(7, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model definition\n",
    "import torch.nn as nn\n",
    "\n",
    "class LAI_LSTM(nn.Module):\n",
    "    def __init__(self, num_features=7, hidden_size=64, num_layers=1):\n",
    "        super(LAI_LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Debug print statements\n",
    "        #print(\"\\n===== FORWARD PASS DEBUG =====\")\n",
    "        #print(\"Input to LSTM:\", x.shape)\n",
    "\n",
    "        # Run LSTM\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        #print(\"LSTM output:\", output.shape)      # (batch, seq_len, hidden)\n",
    "        #print(\"h_n:\", h_n.shape)                 # (num_layers, batch, hidden)\n",
    "        #print(\"c_n:\", c_n.shape)\n",
    "\n",
    "        # Fully connected layer on last hidden state\n",
    "        out = self.fc(h_n[-1])\n",
    "        #print(\"After FC:\", out.shape)            # (batch, 1)\n",
    "\n",
    "        # Squeeze last dimension only\n",
    "        out = out.squeeze(1)\n",
    "        #print(\"Final output:\", out.shape)        # (batch,)\n",
    "\n",
    "        #print(\"===== END FORWARD DEBUG =====\\n\")\n",
    "        return out\n",
    "\n",
    "model = LAI_LSTM(num_features=7, hidden_size=64, num_layers=1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "334a6dda-9854-4c7d-90d9-db5c3f41c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer (ADAM), and the evaluation criterion: MSELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78bb50-8516-4dc1-b8fa-966bd8b18d4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## debugging, ignore! DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80f06f73-e28e-49cc-ad76-c8c9d24180a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2159688, 6, 7)\n",
      "y_train shape: (2159688,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train_np.shape)\n",
    "print(\"y_train shape:\", y_train_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8362d9f0-b5e5-4c86-8c74-db1588ccf0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: torch.Size([32, 6, 7])\n",
      "Batch y: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## de-bugging!! ignore\n",
    "# ----- RUN ONE BATCH ONLY -----\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch X:\", X_batch.shape)\n",
    "print(\"Batch y:\", y_batch.shape)\n",
    "\n",
    "_ = model(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abc937d8-3ff7-441c-8134-52369c7ff7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FORWARD PASS DEBUG =====\n",
      "Input to LSTM: torch.Size([32, 6, 7])\n",
      "LSTM output: torch.Size([32, 6, 64])\n",
      "h_n: torch.Size([1, 32, 64])\n",
      "c_n: torch.Size([1, 32, 64])\n",
      "After FC: torch.Size([32, 1])\n",
      "Final output: torch.Size([32])\n",
      "===== END FORWARD DEBUG =====\n",
      "\n",
      "outputs: torch.Size([32])\n",
      "y_batch: torch.Size([32])\n",
      "LOSS INPUT SHAPES → torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# debugging training loop!! ignore!!\n",
    "for X_batch, y_batch in train_loader:\n",
    "    X_batch = X_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_batch)\n",
    "\n",
    "    print(\"outputs:\", outputs.shape)\n",
    "    print(\"y_batch:\", y_batch.shape)\n",
    "\n",
    "    loss = criterion(outputs, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"LOSS INPUT SHAPES →\", outputs.shape, y_batch.shape)\n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078be424-9da0-4997-a245-78bce9950e68",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a414dd7-d9c4-4454-8cf8-409d526441e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  | Train Loss: 0.3611  | Val Loss: 1.8247\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LAI_LSTM(num_features=7, hidden_size=64, num_layers=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ===== TRAINING =====\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)        # (batch, seq, features)\n",
    "        y_batch = y_batch.to(device)        # (batch,)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X_batch)             # → (batch,)\n",
    "        loss = criterion(y_pred, y_batch)   # matches perfectly\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * len(X_batch)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ===== VALIDATION =====\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in valid_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item() * len(X_batch)\n",
    "\n",
    "    val_loss /= len(valid_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  \"\n",
    "          f\"| Train Loss: {train_loss:.4f}  \"\n",
    "          f\"| Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0595f3-d313-4b15-9e28-5a8db8767aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_losses_mse, val_losses_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbdd59-fd74-4cef-a94c-db1befb98570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export and SAVE model\n",
    "model_path = os.path.join(cwd,'saved_model_X')\n",
    "make_dir(model_path)\n",
    "\n",
    "# Save the model weights to a pth file.\n",
    "torch.save(model.state_dict(), os.path.join(model_path,'LSTM_model_weights_.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750390e-5c7e-4a2b-b69a-d5aa09f843bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### MISSING TASKS/NEXT STEPS\n",
    "## 1 figure out data splitting\n",
    "Currently, I am splitting our data into training and validation at the start of the script and using the timesteps as my splits to have 15 years of training data, 4 years of validation data, which can then be split into validation and training data\n",
    "The example LSTM script brings in the training and testing data separatley, and then gets the validation data from the training data...\n",
    "So, I am unsure if I should:\n",
    "a) go back to the start of the script and split 90% to training, 10% to testing, and then from that 90% generate my training data after all the processing\n",
    "b) Leave my splitting as is, and split my validation data (currently 20% of my data) into 10% validation, 10% testing\n",
    "Note that option b) means that the testing data has been standardized/scaled alongside the validation data. I am unclear if this is an issue.\n",
    "\n",
    "## 2 define the model architecture by setting hyperparameters\n",
    "\n",
    "## 3 actually define the model and the optimizer (ADAM)\n",
    "\n",
    "## 4 run the model and save!\n",
    "\n",
    "## evaluate the trained model on the testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf56d1-eacc-4732-9462-8b01a1dc78c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
